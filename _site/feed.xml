<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://shredlabcmu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shredlabcmu.github.io/" rel="alternate" type="text/html" /><updated>2022-11-09T22:15:59-05:00</updated><id>https://shredlabcmu.github.io/feed.xml</id><title type="html">SHREDLab</title><subtitle>Researching, developing, and testing autonomous robots at Carnegie Mellon University
</subtitle><entry><title type="html">Haptic Guidance Robot</title><link href="https://shredlabcmu.github.io/guidance/" rel="alternate" type="text/html" title="Haptic Guidance Robot" /><published>2022-05-20T06:40:07-04:00</published><updated>2022-05-20T06:40:07-04:00</updated><id>https://shredlabcmu.github.io/haptic-guidance</id><content type="html" xml:base="https://shredlabcmu.github.io/guidance/">&lt;p&gt;Haptic Guidance Robot&lt;/p&gt;

&lt;p&gt;Haptic Guidance Robot research objective is to understand bidirectional nonverbal communications between humans and robots through haptic modalities for guidance purposes. The project aims to build a full-sized guidance robot with a variety of haptic sensors and actuators, and perform studies with human subjects, particularly the visually impaired people, for performance and design evaluation of effective guidance tasks.&lt;/p&gt;

&lt;p&gt;The Haptic Guidance Robot project researches human-robot interaction through the sense of touch. The robot uses haptic modalities for guidance purposes for the visually impaired community.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-03-07-haptic-guidance/Hand_render.JPG&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;publications&quot;&gt;Publications&lt;/h3&gt;
&lt;p&gt;Pending&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-haptic-guidance/Robot_render.JPG&quot; /&gt;
    &lt;figcaption&gt;
        Robot.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-haptic-guidance/RobotWithHuman_render.JPG&quot; /&gt;
    &lt;figcaption&gt;
        Simulation of the human-robot interaction.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Melisa Orta Martinez (https://ri.cmu.edu) - (mortamar [at] andrew [dot] cmu [dot] edu)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">Haptic Guidance Robot</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-03-07-haptic-guidance/Robot_render.JPG" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-03-07-haptic-guidance/Robot_render.JPG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Soft Magnetic Actuator</title><link href="https://shredlabcmu.github.io/soft/" rel="alternate" type="text/html" title="Soft Magnetic Actuator" /><published>2022-05-20T06:40:07-04:00</published><updated>2022-05-20T06:40:07-04:00</updated><id>https://shredlabcmu.github.io/soft-magnetic-actuators</id><content type="html" xml:base="https://shredlabcmu.github.io/soft/">&lt;p&gt;Soft Magnetic Actuator for Haptics.&lt;/p&gt;

&lt;p&gt;Current tactile haptic displays are either able to render tangential and normal forces in one area of actuation on the fingertip, or normal forces in multiple areas of independent actuation, but cannot provide different kinds of forces in multiple independent areas of actuation. We are currently fabricating actuators using very low modulus materials together with magnetic particles to create weakly polarized flexible magnets. These materials respond with attraction or repulsion depending on the direction of the applied magnetic field and can apply shear or normal forces depending on the placement of external electromagnets. We are also developing apparatuses in which to test these materials for efficacy in texture rendering.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-11-08-soft-magnetic-actuators/fingertip_and_magnets.png&quot; alt=&quot;Tiltrotor VTOL&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;publications&quot;&gt;Publications&lt;/h3&gt;
&lt;p&gt;Pending&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-11-08-soft-magnetic-actuators/attracting_repulsing.gif&quot; /&gt;
    &lt;figcaption&gt;
        Interaction with the magneting field.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-11-08-soft-magnetic-actuators/closer_magnet_ruler.gif&quot; /&gt;
    &lt;figcaption&gt;
        Interaction with the magneting field.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ri.cmu.edu&quot;&gt;Sarah Costrell (MechE)&lt;/a&gt; (mail [at] cmu [dot] edu)&lt;/li&gt;
  &lt;li&gt;Melisa Orta Martinez (https://ri.cmu.edu) - (mortamar [at] andrew [dot] cmu [dot] edu)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">Soft Magnetic Actuator for Haptics.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-11-08-soft-magnetic-actuators/fingertipandmagnet.jpg" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-11-08-soft-magnetic-actuators/fingertipandmagnet.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">RoboLoom</title><link href="https://shredlabcmu.github.io/roboloom/" rel="alternate" type="text/html" title="RoboLoom" /><published>2022-02-06T07:00:00-05:00</published><updated>2022-02-06T07:00:00-05:00</updated><id>https://shredlabcmu.github.io/roboloom</id><content type="html" xml:base="https://shredlabcmu.github.io/roboloom/">&lt;p&gt;RoboLoom is a project to design, build, and deploy a robotic fully jacquard loom that has software interfaces to aid in teaching matrix math through weaving.&lt;/p&gt;

&lt;p&gt;Most people don’t know how closely related computers, matrices, and the craft of weaving are, but weaving patterns and early loom designs inspired some of early computing and matrices. For today’s looms and computers there is still a connection. Matrices can describe how a loom is threaded, and what actions are taken each time step while weaving. These matrices can then be multiplied together resulting in a cloth’s pattern. Designing and optimizing these matrices can be explored as a computational problem in a crafting framework. RoboLoom aims to explore this and bring weaving and matrix math together in new and interesting ways.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-roboloom/RoboLoom2&quot; /&gt;
    &lt;figcaption&gt;
        RoboLoom assembled.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/yuheng/&quot;&gt;Samantha Speer&lt;/a&gt; &amp;lt;yuhengq [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;Wenshan Wang &amp;lt;wenshanw [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yuheng Qiu</name></author><category term="research" /><summary type="html">RoboLoom is a project to design, build, and deploy a robotic fully jacquard loom that has software interfaces to aid in teaching matrix math through weaving.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-02-06-roboloom/RoboLoom1.jpg" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-02-06-roboloom/RoboLoom1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Robot-Mediated haptics for blind students STEM education</title><link href="https://shredlabcmu.github.io/aircode/" rel="alternate" type="text/html" title="Robot-Mediated haptics for blind students STEM education" /><published>2021-10-06T08:00:00-04:00</published><updated>2021-10-06T08:00:00-04:00</updated><id>https://shredlabcmu.github.io/template%20copy</id><content type="html" xml:base="https://shredlabcmu.github.io/aircode/">&lt;p&gt;Blind and low vision STEM education is hindered by an inability to engage with visual representations of STEM concepts. Our sense of touch provides us with a separate, distributed channel that could allow us to convey abstract STEM concepts non-visually. We propose using robot-mediated haptics to enable blind and low vision students to engage in STEM education through hands-on, interactive learning.&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ri.cmu.edu&quot;&gt;Sarah Costrell (MechE)&lt;/a&gt; (mail [at] cmu [dot] edu)&lt;/li&gt;
  &lt;li&gt;Melisa Orta Martinez (https://ri.cmu.edu) - (mortamar [at] andrew [dot] cmu [dot] edu)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">Blind and low vision STEM education is hindered by an inability to engage with visual representations of STEM concepts. Our sense of touch provides us with a separate, distributed channel that could allow us to convey abstract STEM concepts non-visually. We propose using robot-mediated haptics to enable blind and low vision students to engage in STEM education through hands-on, interactive learning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-05-02-haptic-blind/blind-guide.png" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-05-02-haptic-blind/blind-guide.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>