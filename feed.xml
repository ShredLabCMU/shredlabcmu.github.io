<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://shredlabcmu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://shredlabcmu.github.io/" rel="alternate" type="text/html" /><updated>2023-09-29T17:48:44-04:00</updated><id>https://shredlabcmu.github.io/feed.xml</id><title type="html">SHREDLab</title><subtitle>Researching haptic technologies with social and educational impact at Carnegie Mellon University
</subtitle><entry><title type="html">HazelWood Workshop</title><link href="https://shredlabcmu.github.io/hazelwood/" rel="alternate" type="text/html" title="HazelWood Workshop" /><published>2023-09-29T08:00:00-04:00</published><updated>2023-09-29T08:00:00-04:00</updated><id>https://shredlabcmu.github.io/Hazelwood</id><content type="html" xml:base="https://shredlabcmu.github.io/hazelwood/">&lt;p&gt;Thanks to the Build Back Better federal grant awarded to the South-Western
Pennsylvania Region and collaboration with the Regional Industrial Development Corporation
(RIDC)- Carnegie Mellon University has recently announced the renovation of an old steel mill,
Mill 19, into a new $150 million robotics manufacturing facility in the heart of the Hazelwood
community. In response to this new construction, the Social Haptics Robotics and Education
(SHRED) Laboratory is supporting local Hazelwood youth through a community workshop series
called the Art of Sound. We are motivated to preserve existing community members’ sense of
belonging by installing multiple art pieces that describe the stories and experiences of those
local to the region.&lt;/p&gt;

&lt;h2 id=&quot;what-is-it-about&quot;&gt;What is it about?&lt;/h2&gt;
&lt;p&gt;In this active monthly workshop series, we guide middle-school and early high school
students in constructing five distinct interactive artwork installations. These five paintings
utilize capacitive touch technology to play various noises and sounds, correlating to each
painting’s unique theme. In conversation with the community stakeholders prior to this work,
they outlined the local Hazelwood library, coffeeshop, and family support center as the most
impactful locations to display their youth’s hard work. With the support of local artist Edith
Abeyta and singer Erika Johnson, we aim to leverage an interdisciplinary approach to support
their sustained engagement with STEM learning. This work contributes to the over-arching
research of the SHRED Lab’s community outreach personnel Will Scott, whose research focuses
on decentering educational technology from its colonial heritage by leveraging historically
“feminized” practices from artistic and craft-based disciplines. This series will run for from the
Fall 2023 and Spring 2024 semesters.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2023-09-29-hazelwood-center/group.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Students working at the Hazelwood library.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/team/will/&quot;&gt;William Scott&lt;/a&gt; &amp;lt;wscott2 [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>William Scott</name></author><category term="research" /><category term="highlights" /><summary type="html">Thanks to the Build Back Better federal grant awarded to the South-Western Pennsylvania Region and collaboration with the Regional Industrial Development Corporation (RIDC)- Carnegie Mellon University has recently announced the renovation of an old steel mill, Mill 19, into a new $150 million robotics manufacturing facility in the heart of the Hazelwood community. In response to this new construction, the Social Haptics Robotics and Education (SHRED) Laboratory is supporting local Hazelwood youth through a community workshop series called the Art of Sound. We are motivated to preserve existing community members’ sense of belonging by installing multiple art pieces that describe the stories and experiences of those local to the region.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2023-09-29-hazelwood-center/group.jpg" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2023-09-29-hazelwood-center/group.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">RoboLoom: Teach math through weaving</title><link href="https://shredlabcmu.github.io/roboloom/" rel="alternate" type="text/html" title="RoboLoom: Teach math through weaving" /><published>2022-05-20T06:40:07-04:00</published><updated>2022-05-20T06:40:07-04:00</updated><id>https://shredlabcmu.github.io/roboloom</id><content type="html" xml:base="https://shredlabcmu.github.io/roboloom/">&lt;p&gt;RoboLoom is a project to design, build, and deploy a robotic fully jacquard loom that has software interfaces to aid in teaching matrix math through weaving.&lt;/p&gt;

&lt;p&gt;Most people don’t know how closely related computers, matrices, and the craft of weaving are, but weaving patterns and early loom designs inspired some of early computing and matrices. For today’s looms and computers there is still a connection. Matrices can describe how a loom is threaded, and what actions are taken each time step while weaving. These matrices can then be multiplied together resulting in a cloth’s pattern. Designing and optimizing these matrices can be explored as a computational problem in a crafting framework. RoboLoom aims to explore this and bring weaving and matrix math together in new and interesting ways.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-roboloom/RoboLoom2.jpg&quot; /&gt;
    &lt;figcaption&gt;
        RoboLoom assembled.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-roboloom/RoboLoom4.jpg&quot; /&gt;
    &lt;figcaption&gt;
        RoboLoom assembled.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-roboloom/RoboLoom_Render1.PNG&quot; /&gt;
    &lt;figcaption&gt;
        Render of the RoboLoom.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;mailto:snspeer@andrew.cmu.edu&quot;&gt;Samantha Speer&lt;/a&gt; &amp;lt;snspeer [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">RoboLoom is a project to design, build, and deploy a robotic fully jacquard loom that has software interfaces to aid in teaching matrix math through weaving.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-02-06-roboloom/RoboLoom4.jpg" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-02-06-roboloom/RoboLoom4.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Haptic Guidance Robot</title><link href="https://shredlabcmu.github.io/guidance/" rel="alternate" type="text/html" title="Haptic Guidance Robot" /><published>2022-05-20T06:40:07-04:00</published><updated>2022-05-20T06:40:07-04:00</updated><id>https://shredlabcmu.github.io/haptic-guidance</id><content type="html" xml:base="https://shredlabcmu.github.io/guidance/">&lt;p&gt;Haptic Guidance Robot&lt;/p&gt;

&lt;p&gt;Haptic Guidance Robot research objective is to understand bidirectional nonverbal communications between humans and robots through haptic modalities for guidance purposes. The project aims to build a full-sized guidance robot with a variety of haptic sensors and actuators, and perform studies with human subjects, particularly the visually impaired people, for performance and design evaluation of effective guidance tasks.&lt;/p&gt;

&lt;p&gt;The Haptic Guidance Robot project researches human-robot interaction through the sense of touch. The robot uses haptic modalities for guidance purposes for the visually impaired community.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-03-07-haptic-guidance/Hand_render.JPG&quot; /&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-haptic-guidance/Robot_render.JPG&quot; /&gt;
    &lt;figcaption&gt;
        Robot Rendering.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-07-haptic-guidance/RobotWithHuman_render.JPG&quot; /&gt;
    &lt;figcaption&gt;
        Simulation of the human-robot interaction.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https:shredlabcmu.github.io/team/abena&quot;&gt;Abena Boadi-Agyemang&lt;/a&gt; - &lt;a href=&quot;mailto:abodiag@andrew.cmu.edu&quot;&gt;abodiag@andrew.cmu.edu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">Haptic Guidance Robot</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-03-07-haptic-guidance/Robot_render.JPG" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-03-07-haptic-guidance/Robot_render.JPG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Soft Magnetic Actuator</title><link href="https://shredlabcmu.github.io/soft/" rel="alternate" type="text/html" title="Soft Magnetic Actuator" /><published>2022-05-20T06:40:07-04:00</published><updated>2022-05-20T06:40:07-04:00</updated><id>https://shredlabcmu.github.io/soft-magnetic-actuators</id><content type="html" xml:base="https://shredlabcmu.github.io/soft/">&lt;p&gt;Soft Magnetic Actuator for Haptics.&lt;/p&gt;

&lt;p&gt;Current tactile haptic displays are either able to render tangential and normal forces in one area of actuation on the fingertip, or normal forces in multiple areas of independent actuation, but cannot provide different kinds of forces in multiple independent areas of actuation. We are currently fabricating actuators using very low modulus materials together with magnetic particles to create weakly polarized flexible magnets. These materials respond with attraction or repulsion depending on the direction of the applied magnetic field and can apply shear or normal forces depending on the placement of external electromagnets. We are also developing apparatuses in which to test these materials for efficacy in texture rendering.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2022-11-08-soft-magnetic-actuators/fingertip_and_magnets.png&quot; alt=&quot;Tiltrotor VTOL&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;publications&quot;&gt;Publications&lt;/h3&gt;
&lt;p&gt;Pending&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-11-08-soft-magnetic-actuators/attracting_repulsing.gif&quot; /&gt;
    &lt;figcaption&gt;
        Interaction with the magneting field.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-11-08-soft-magnetic-actuators/closer_magnet_ruler.gif&quot; /&gt;
    &lt;figcaption&gt;
        Interaction with the magneting field.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sarah Costrell (MechE) &lt;a href=&quot;mailto:scostrel@andrew.cmu.edu&quot;&gt;(scostrel@andrew.cmu.edu)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">Soft Magnetic Actuator for Haptics.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-11-08-soft-magnetic-actuators/fingertipandmagnet.jpg" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-11-08-soft-magnetic-actuators/fingertipandmagnet.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Robot-Mediated haptics for blind students STEM education</title><link href="https://shredlabcmu.github.io/aircode/" rel="alternate" type="text/html" title="Robot-Mediated haptics for blind students STEM education" /><published>2021-10-06T08:00:00-04:00</published><updated>2021-10-06T08:00:00-04:00</updated><id>https://shredlabcmu.github.io/template%20copy</id><content type="html" xml:base="https://shredlabcmu.github.io/aircode/">&lt;p&gt;Blind and low vision STEM education is hindered by an inability to engage with visual representations of STEM concepts. Our sense of touch provides us with a separate, distributed channel that could allow us to convey abstract STEM concepts non-visually. We propose using robot-mediated haptics to enable blind and low vision students to engage in STEM education through hands-on, interactive learning.&lt;/p&gt;

&lt;h3 id=&quot;contact&quot;&gt;Contact&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ri.cmu.edu&quot;&gt;Sarah Costrell (MechE)&lt;/a&gt; (mail [at] cmu [dot] edu)&lt;/li&gt;
  &lt;li&gt;Melisa Orta Martinez (https://ri.cmu.edu) - (mortamar [at] andrew [dot] cmu [dot] edu)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>SHRED Lab</name></author><category term="research" /><summary type="html">Blind and low vision STEM education is hindered by an inability to engage with visual representations of STEM concepts. Our sense of touch provides us with a separate, distributed channel that could allow us to convey abstract STEM concepts non-visually. We propose using robot-mediated haptics to enable blind and low vision students to engage in STEM education through hands-on, interactive learning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://shredlabcmu.github.io/img/posts/2022-05-02-haptic-blind/blind-guide.png" /><media:content medium="image" url="https://shredlabcmu.github.io/img/posts/2022-05-02-haptic-blind/blind-guide.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>